{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the hidden layer\n",
    "\n",
    "##### Prerequisites\n",
    "\n",
    "Below, we are going to walk through the math of neural networks in a multilayer perceptron. With multiple perceptrons, we are going to move to using vectors and matrices. To brush up, be sure to view the following:\n",
    "\n",
    "1. Khan Academy's [introduction to vectors](https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/vectors/v/vector-introduction-linear-algebra).\n",
    "2. Khan Academy's [introduction to matrices](https://www.khanacademy.org/math/precalculus/precalc-matrices).\n",
    "\n",
    "##### Derivation\n",
    "\n",
    "Before, we were dealing with only one output node which made the code straightforward. However now that we have multiple input units and multiple hidden units, the weights between them will require two indices: w_{ij}wij where ii denotes input units and jj are the hidden units.\n",
    "\n",
    "For example, the following image shows our network, with its input units labeled x_1, x_2,x1,x2, and x_3x3, and its hidden nodes labeled h_1h1 and h_2h2:\n",
    "\n",
    "[![img](https://d17h27t6h515a5.cloudfront.net/topher/2017/February/589973b5_network-with-labeled-nodes/network-with-labeled-nodes.png)](https://classroom.udacity.com/nanodegrees/nd101-ent/parts/7de57e8c-12ed-4d7a-b8ea-15db419f6a58/modules/9f27732b-a272-4d8d-8cf3-28159ebc7200/lessons/85c95c4c-4b3a-42d8-983b-9f760fe38055/concepts/7d0a1958-be25-4efb-ab81-360d9aa4f764#)\n",
    "\n",
    "The lines indicating the weights leading to h_1h1 have been colored differently from those leading to h_2h2just to make it easier to read.\n",
    "\n",
    "Now to index the weights, we take the input unit number for the _ii and the hidden unit number for the _j.j. That gives us\n",
    "\n",
    "w_{11}w11\n",
    "\n",
    "for the weight leading from x_1x1 to h_1h1, and\n",
    "\n",
    "w_{12}w12\n",
    "\n",
    "for the weight leading from x_1x1 to h_2h2.\n",
    "\n",
    "The following image includes all of the weights between the input layer and the hidden layer, labeled with their appropriate w_{ij}wij indices:\n",
    "\n",
    "[![img](https://d17h27t6h515a5.cloudfront.net/topher/2017/February/589978f4_network-with-labeled-weights/network-with-labeled-weights.png)](https://classroom.udacity.com/nanodegrees/nd101-ent/parts/7de57e8c-12ed-4d7a-b8ea-15db419f6a58/modules/9f27732b-a272-4d8d-8cf3-28159ebc7200/lessons/85c95c4c-4b3a-42d8-983b-9f760fe38055/concepts/7d0a1958-be25-4efb-ab81-360d9aa4f764#)\n",
    "\n",
    "Before, we were able to write the weights as an array, indexed as w_iwi.\n",
    "\n",
    "But now, the weights need to be stored in a **matrix**, indexed as w_{ij}wij. Each **row** in the matrix will correspond to the weights **leading out** of a **single input unit**, and each **column** will correspond to the weights **leading in** to a **single hidden unit**. For our three input units and two hidden units, the weights matrix looks like this:\n",
    "\n",
    "[![img](https://d17h27t6h515a5.cloudfront.net/topher/2017/February/58a49908_multilayer-diagram-weights/multilayer-diagram-weights.png)Weights matrix for 3 input units and 2 hidden units](https://classroom.udacity.com/nanodegrees/nd101-ent/parts/7de57e8c-12ed-4d7a-b8ea-15db419f6a58/modules/9f27732b-a272-4d8d-8cf3-28159ebc7200/lessons/85c95c4c-4b3a-42d8-983b-9f760fe38055/concepts/7d0a1958-be25-4efb-ab81-360d9aa4f764#)\n",
    "\n",
    "Be sure to compare the matrix above with the diagram shown before it so you can see where the different weights in the network end up in the matrix.\n",
    "\n",
    "To initialize these weights in NumPy, we have to provide the shape of the matrix. If `features` is a 2D array containing the input data:\n",
    "\n",
    "```\n",
    "# Number of records and input units\n",
    "n_records, n_inputs = features.shape\n",
    "# Number of hidden units\n",
    "n_hidden = 2\n",
    "weights_input_to_hidden = np.random.normal(0, n_inputs**-0.5, size=(n_inputs, n_hidden))\n",
    "\n",
    "```\n",
    "\n",
    "This creates a 2D array (i.e. a matrix) named `weights_input_to_hidden` with dimensions `n_inputs` by `n_hidden`. Remember how the input to a hidden unit is the sum of all the inputs multiplied by the hidden unit's weights. So for each hidden layer unit, h_jhj, we need to calculate the following:\n",
    "\n",
    "[![img](https://d17h27t6h515a5.cloudfront.net/topher/2017/February/589958d5_hidden-layer-weights/hidden-layer-weights.gif)](https://classroom.udacity.com/nanodegrees/nd101-ent/parts/7de57e8c-12ed-4d7a-b8ea-15db419f6a58/modules/9f27732b-a272-4d8d-8cf3-28159ebc7200/lessons/85c95c4c-4b3a-42d8-983b-9f760fe38055/concepts/7d0a1958-be25-4efb-ab81-360d9aa4f764#)\n",
    "\n",
    "To do that, we now need to use [matrix multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication). If your linear algebra is rusty, I suggest taking a look at the suggested resources in the prerequisites section. For this part though, you'll only need to know how to multiply a matrix with a vector.\n",
    "\n",
    "In this case, we're multiplying the inputs (a row vector here) by the weights. To do this, you take the dot (inner) product of the inputs with each column in the weights matrix. For example, to calculate the input to the first hidden unit, j = 1j=1, you'd take the dot product of the inputs with the first column of the weights matrix, like so:\n",
    "\n",
    "[![img](https://d17h27t6h515a5.cloudfront.net/topher/2017/January/58895788_input-times-weights/input-times-weights.png)Calculating the input to the first hidden unit with the first column of the weights matrix.](https://classroom.udacity.com/nanodegrees/nd101-ent/parts/7de57e8c-12ed-4d7a-b8ea-15db419f6a58/modules/9f27732b-a272-4d8d-8cf3-28159ebc7200/lessons/85c95c4c-4b3a-42d8-983b-9f760fe38055/concepts/7d0a1958-be25-4efb-ab81-360d9aa4f764#)\n",
    "\n",
    "[![img](https://d17h27t6h515a5.cloudfront.net/topher/2017/January/588ae392_codecogseqn-2/codecogseqn-2.png)](https://classroom.udacity.com/nanodegrees/nd101-ent/parts/7de57e8c-12ed-4d7a-b8ea-15db419f6a58/modules/9f27732b-a272-4d8d-8cf3-28159ebc7200/lessons/85c95c4c-4b3a-42d8-983b-9f760fe38055/concepts/7d0a1958-be25-4efb-ab81-360d9aa4f764#)\n",
    "\n",
    "And for the second hidden layer input, you calculate the dot product of the inputs with the second column. And so on and so forth.\n",
    "\n",
    "In NumPy, you can do this for all the inputs and all the outputs at once using `np.dot`\n",
    "\n",
    "```\n",
    "hidden_inputs = np.dot(inputs, weights_input_to_hidden)\n",
    "\n",
    "```\n",
    "\n",
    "You could also define your weights matrix such that it has dimensions `n_hidden` by `n_inputs` then multiply like so where the inputs form a *column vector*:\n",
    "\n",
    "[![img](https://d17h27t6h515a5.cloudfront.net/topher/2017/January/588b7c74_inputs-matrix/inputs-matrix.png)](https://classroom.udacity.com/nanodegrees/nd101-ent/parts/7de57e8c-12ed-4d7a-b8ea-15db419f6a58/modules/9f27732b-a272-4d8d-8cf3-28159ebc7200/lessons/85c95c4c-4b3a-42d8-983b-9f760fe38055/concepts/7d0a1958-be25-4efb-ab81-360d9aa4f764#)\n",
    "\n",
    "**Note:** The weight indices have changed in the above image and no longer match up with the labels used in the earlier diagrams. That's because, in matrix notation, the row index always precedes the column index, so it would be misleading to label them the way we did in the neural net diagram. Just keep in mind that this is the same weight matrix as before, but rotated so the first column is now the first row, and the second column is now the second row. If we *were* to use the labels from the earlier diagram, the weights would fit into the matrix in the following locations:\n",
    "\n",
    "[![img](https://d17h27t6h515a5.cloudfront.net/topher/2017/February/589acab9_weight-label-reference/weight-label-reference.gif)Weight matrix shown with labels matching earlier diagrams.](https://classroom.udacity.com/nanodegrees/nd101-ent/parts/7de57e8c-12ed-4d7a-b8ea-15db419f6a58/modules/9f27732b-a272-4d8d-8cf3-28159ebc7200/lessons/85c95c4c-4b3a-42d8-983b-9f760fe38055/concepts/7d0a1958-be25-4efb-ab81-360d9aa4f764#)\n",
    "\n",
    "Remember, the above is **not** a correct view of the **indices**, but it uses the labels from the earlier neural net diagrams to show you where each weight ends up in the matrix.\n",
    "\n",
    "The important thing with matrix multiplication is that *the dimensions match*. For matrix multiplication to work, there has to be the same number of elements in the dot products. In the first example, there are three columns in the input vector, and three rows in the weights matrix. In the second example, there are three columns in the weights matrix and three rows in the input vector. If the dimensions don't match, you'll get this:\n",
    "\n",
    "```\n",
    "# Same weights and features as above, but swapped the order\n",
    "hidden_inputs = np.dot(weights_input_to_hidden, features)\n",
    "---------------------------------------------------------------------------\n",
    "ValueError                                Traceback (most recent call last)\n",
    "<ipython-input-11-1bfa0f615c45> in <module>()\n",
    "----> 1 hidden_in = np.dot(weights_input_to_hidden, X)\n",
    "\n",
    "ValueError: shapes (3,2) and (3,) not aligned: 2 (dim 1) != 3 (dim 0)\n",
    "\n",
    "```\n",
    "\n",
    "The dot product can't be computed for a 3x2 matrix and 3-element array. That's because the 2 columns in the matrix don't match the number of elements in the array. Some of the dimensions that could work would be the following:\n",
    "\n",
    "[![img](https://d17h27t6h515a5.cloudfront.net/topher/2017/February/58924a8d_matrix-mult-3/matrix-mult-3.png)](https://classroom.udacity.com/nanodegrees/nd101-ent/parts/7de57e8c-12ed-4d7a-b8ea-15db419f6a58/modules/9f27732b-a272-4d8d-8cf3-28159ebc7200/lessons/85c95c4c-4b3a-42d8-983b-9f760fe38055/concepts/7d0a1958-be25-4efb-ab81-360d9aa4f764#)\n",
    "\n",
    "The rule is that if you're multiplying an array from the left, the array must have the same number of elements as there are rows in the matrix. And if you're multiplying the *matrix* from the left, the number of columns in the matrix must equal the number of elements in the array on the right.\n",
    "\n",
    "### Making a column vector\n",
    "\n",
    "You see above that sometimes you'll want a column vector, even though by default NumPy arrays work like row vectors. It's possible to get the transpose of an array like so `arr.T`, but for a 1D array, the transpose will return a row vector. Instead, use `arr[:,None]` to create a column vector:\n",
    "\n",
    "```\n",
    "print(features)\n",
    "> array([ 0.49671415, -0.1382643 ,  0.64768854])\n",
    "\n",
    "print(features.T)\n",
    "> array([ 0.49671415, -0.1382643 ,  0.64768854])\n",
    "\n",
    "print(features[:, None])\n",
    "> array([[ 0.49671415],\n",
    "       [-0.1382643 ],\n",
    "       [ 0.64768854]])\n",
    "\n",
    "```\n",
    "\n",
    "Alternatively, you can create arrays with two dimensions. Then, you can use `arr.T` to get the column vector.\n",
    "\n",
    "```\n",
    "np.array(features, ndmin=2)\n",
    "> array([[ 0.49671415, -0.1382643 ,  0.64768854]])\n",
    "\n",
    "np.array(features, ndmin=2).T\n",
    "> array([[ 0.49671415],\n",
    "       [-0.1382643 ],\n",
    "       [ 0.64768854]])\n",
    "\n",
    "```\n",
    "\n",
    "I personally prefer keeping all vectors as 1D arrays, it just works better in my head.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "Now we've come to the problem of how to make a multilayer neural network *learn*. Before, we saw how to update weights with gradient descent. The backpropagation algorithm is just an extension of that, using the chain rule to find the error with the respect to the weights connecting the input layer to the hidden layer (for a two layer network).\n",
    "\n",
    "To update the weights to hidden layers using gradient descent, you need to know how much error each of the hidden units contributed to the final output. Since the output of a layer is determined by the weights between layers, the error resulting from units is scaled by the weights going forward through the network. Since we know the error at the output, we can use the weights to work backwards to hidden layers.\n",
    "\n",
    "For example, in the output layer, you have errors \\delta^o_kδko attributed to each output unit kk. Then, the error attributed to hidden unit jj is the output errors, scaled by the weights between the output and hidden layers (and the gradient):\n",
    "\n",
    "[![img](https://d17h27t6h515a5.cloudfront.net/topher/2017/January/588bc0c6_backprop-error/backprop-error.gif)](https://classroom.udacity.com/nanodegrees/nd101-ent/parts/7de57e8c-12ed-4d7a-b8ea-15db419f6a58/modules/9f27732b-a272-4d8d-8cf3-28159ebc7200/lessons/85c95c4c-4b3a-42d8-983b-9f760fe38055/concepts/87d85ff2-db15-438b-9be8-d097ea917f1e#)\n",
    "\n",
    "Then, the gradient descent step is the same as before, just with the new errors:\n",
    "\n",
    "[![img](https://d17h27t6h515a5.cloudfront.net/topher/2017/January/588bc23b_backprop-weight-update/backprop-weight-update.gif)](https://classroom.udacity.com/nanodegrees/nd101-ent/parts/7de57e8c-12ed-4d7a-b8ea-15db419f6a58/modules/9f27732b-a272-4d8d-8cf3-28159ebc7200/lessons/85c95c4c-4b3a-42d8-983b-9f760fe38055/concepts/87d85ff2-db15-438b-9be8-d097ea917f1e#)\n",
    "\n",
    "where w_{ij}wij are the weights between the inputs and hidden layer and x_ixi are input unit values. This form holds for however many layers there are. The weight steps are equal to the step size times the output error of the layer times the values of the inputs to that layer\n",
    "\n",
    "[![img](https://d17h27t6h515a5.cloudfront.net/topher/2017/January/588bc2d4_backprop-general/backprop-general.gif)](https://classroom.udacity.com/nanodegrees/nd101-ent/parts/7de57e8c-12ed-4d7a-b8ea-15db419f6a58/modules/9f27732b-a272-4d8d-8cf3-28159ebc7200/lessons/85c95c4c-4b3a-42d8-983b-9f760fe38055/concepts/87d85ff2-db15-438b-9be8-d097ea917f1e#)\n",
    "\n",
    "Here, you get the output error, \\delta_{output}δoutput, by propagating the errors backwards from higher layers. And the input values, V_{in}Vin are the inputs to the layer, the hidden layer activations to the output unit for example.\n",
    "\n",
    "### Working through an example\n",
    "\n",
    "Let's walk through the steps of calculating the weight updates for a simple two layer network. Suppose there are two input values, one hidden unit, and one output unit, with sigmoid activations on the hidden and output units. The following image depicts this network. (**Note:** the input values are shown as nodes at the bottom of the image, while the network's output value is shown as \\hat yy^ at the top. The inputs themselves do not count as a layer, which is why this is considered a two layer network.)\n",
    "\n",
    "[![img](https://d17h27t6h515a5.cloudfront.net/topher/2017/January/588bb45d_backprop-network/backprop-network.png)](https://classroom.udacity.com/nanodegrees/nd101-ent/parts/7de57e8c-12ed-4d7a-b8ea-15db419f6a58/modules/9f27732b-a272-4d8d-8cf3-28159ebc7200/lessons/85c95c4c-4b3a-42d8-983b-9f760fe38055/concepts/87d85ff2-db15-438b-9be8-d097ea917f1e#)\n",
    "\n",
    "Assume we're trying to fit some binary data and the target is y = 1y=1. We'll start with the forward pass, first calculating the input to the hidden unit\n",
    "\n",
    "h = \\sum_i w_i x_i = 0.1 \\times 0.4 - 0.2 \\times 0.3 = -0.02h=∑iwixi=0.1×0.4−0.2×0.3=−0.02\n",
    "\n",
    "and the output of the hidden unit\n",
    "\n",
    "a = f(h) = \\mathrm{sigmoid}(-0.02) = 0.495a=f(h)=sigmoid(−0.02)=0.495.\n",
    "\n",
    "Using this as the input to the output unit, the output of the network is\n",
    "\n",
    "\\hat y = f(W \\cdot a) = \\mathrm{sigmoid}(0.1 \\times 0.495) = 0.512y^=f(W⋅a)=sigmoid(0.1×0.495)=0.512.\n",
    "\n",
    "With the network output, we can start the backwards pass to calculate the weight updates for both layers. Using the fact that for the sigmoid function f'(W \\cdot a) = f(W \\cdot a) (1 - f(W \\cdot a))f′(W⋅a)=f(W⋅a)(1−f(W⋅a)), the error term for the output unit is\n",
    "\n",
    "\\delta^o = (y - \\hat y) f'(W \\cdot a) = (1 - 0.512) \\times 0.512 \\times(1 - 0.512) = 0.122δo=(y−y^)f′(W⋅a)=(1−0.512)×0.512×(1−0.512)=0.122.\n",
    "\n",
    "Now we need to calculate the error term for the hidden unit with backpropagation. Here we'll scale the error term from the output unit by the weight WW connecting it to the hidden unit. For the hidden unit error term, \\delta^h_j = \\sum_k W_{jk} \\delta^o_k f'(h_j)δjh=∑kWjkδkof′(hj), but since we have one hidden unit and one output unit, this is much simpler.\n",
    "\n",
    "\\delta^h = W \\delta^o f'(h) = 0.1 \\times 0.122 \\times 0.495 \\times (1 - 0.495) = 0.003δh=Wδof′(h)=0.1×0.122×0.495×(1−0.495)=0.003\n",
    "\n",
    "Now that we have the errors, we can calculate the gradient descent steps. The hidden to output weight step is the learning rate, times the output unit error, times the hidden unit activation value.\n",
    "\n",
    "\\Delta W = \\eta \\delta^o a = 0.5 \\times 0.122 \\times 0.495 = 0.0302ΔW=ηδoa=0.5×0.122×0.495=0.0302\n",
    "\n",
    "Then, for the input to hidden weights w_iwi, it's the learning rate times the hidden unit error, times the input values.\n",
    "\n",
    "\\Delta w_i = \\eta \\delta^h x_i = (0.5 \\times 0.003 \\times 0.1, 0.5 \\times 0.003 \\times 0.3) = (0.00015, 0.00045)Δwi=ηδhxi=(0.5×0.003×0.1,0.5×0.003×0.3)=(0.00015,0.00045)\n",
    "\n",
    "From this example, you can see one of the effects of using the sigmoid function for the activations. The maximum derivative of the sigmoid function is 0.25, so the errors in the output layer get reduced by at least 75%, and errors in the hidden layer are scaled down by at least 93.75%! You can see that if you have a lot of layers, using a sigmoid activation function will quickly reduce the weight steps to tiny values in layers near the input. This is known as the **vanishing gradient** problem. Later in the course you'll learn about other activation functions that perform better in this regard and are more commonly used in modern network architectures.\n",
    "\n",
    "## Implementing in NumPy\n",
    "\n",
    "For the most part you have everything you need to implement backpropagation with NumPy.\n",
    "\n",
    "However, previously we were only dealing with error terms from one unit. Now, in the weight update, we have to consider the error for *each unit* in the hidden layer, \\delta_jδj:\n",
    "\n",
    "\\Delta w_{ij} = \\eta \\delta_j x_iΔwij=ηδjxi\n",
    "\n",
    "Firstly, there will likely be a different number of input and hidden units, so trying to multiply the errors and the inputs as row vectors will throw an error:\n",
    "\n",
    "```\n",
    "hidden_error*inputs\n",
    "---------------------------------------------------------------------------\n",
    "ValueError                                Traceback (most recent call last)\n",
    "<ipython-input-22-3b59121cb809> in <module>()\n",
    "----> 1 hidden_error*x\n",
    "\n",
    "ValueError: operands could not be broadcast together with shapes (3,) (6,) \n",
    "\n",
    "```\n",
    "\n",
    "Also, w_{ij}wij is a matrix now, so the right side of the assignment must have the same shape as the left side. Luckily, NumPy takes care of this for us. If you multiply a row vector array with a column vector array, it will multiply the first element in the column by each element in the row vector and set that as the first row in a new 2D array. This continues for each element in the column vector, so you get a 2D array that has shape `(len(column_vector), len(row_vector))`.\n",
    "\n",
    "```\n",
    "hidden_error*inputs[:,None]\n",
    "array([[ -8.24195994e-04,  -2.71771975e-04,   1.29713395e-03],\n",
    "       [ -2.87777394e-04,  -9.48922722e-05,   4.52909055e-04],\n",
    "       [  6.44605731e-04,   2.12553536e-04,  -1.01449168e-03],\n",
    "       [  0.00000000e+00,   0.00000000e+00,  -0.00000000e+00],\n",
    "       [  0.00000000e+00,   0.00000000e+00,  -0.00000000e+00],\n",
    "       [  0.00000000e+00,   0.00000000e+00,  -0.00000000e+00]])\n",
    "\n",
    "```\n",
    "\n",
    "It turns out this is exactly how we want to calculate the weight update step. As before, if you have your inputs as a 2D array with one row, you can also do `hidden_error*inputs.T`, but that won't work if `inputs` is a 1D array.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing backpropagation\n",
    "\n",
    "Now we've seen that the error term for the output layer is\n",
    "\n",
    "\\delta_k = (y_k - \\hat y_k) f'(a_k)δk=(yk−y^k)f′(ak)\n",
    "\n",
    "and the error term for the hidden layer is\n",
    "\n",
    "[![img](https://d17h27t6h515a5.cloudfront.net/topher/2017/January/588bc453_hidden-errors/hidden-errors.gif)](https://classroom.udacity.com/nanodegrees/nd101-ent/parts/7de57e8c-12ed-4d7a-b8ea-15db419f6a58/modules/9f27732b-a272-4d8d-8cf3-28159ebc7200/lessons/85c95c4c-4b3a-42d8-983b-9f760fe38055/concepts/b2bbdc9a-9f48-4735-b408-71cf67f5b000#)\n",
    "\n",
    "For now we'll only consider a simple network with one hidden layer and one output unit. Here's the general algorithm for updating the weights with backpropagation:\n",
    "\n",
    "- Set the weight steps for each layer to zero\n",
    "  - The input to hidden weights \\Delta w_{ij} = 0Δwij=0\n",
    "  - The hidden to output weights \\Delta W_j = 0ΔWj=0\n",
    "- For each record in the training data:\n",
    "  - Make a forward pass through the network, calculating the output \\hat yy^\n",
    "  - Calculate the error gradient in the output unit, \\delta^o = (y - \\hat y) f'(z)δo=(y−y^)f′(z) where z = \\sum_j W_j a_jz=∑jWjaj, the input to the output unit.\n",
    "  - Propagate the errors to the hidden layer \\delta^h_j = \\delta^o W_j f'(h_j)δjh=δoWjf′(hj)\n",
    "  - Update the weight steps:\n",
    "    - \\Delta W_j = \\Delta W_j + \\delta^o a_jΔWj=ΔWj+δoaj\n",
    "    - \\Delta w_{ij} = \\Delta w_{ij} + \\delta^h_j a_iΔwij=Δwij+δjhai\n",
    "- Update the weights, where \\etaη is the learning rate and mm is the number of records:\n",
    "  - W_j = W_j + \\eta \\Delta W_j / mWj=Wj+ηΔWj/m\n",
    "  - w_{ij} = w_{ij} + \\eta \\Delta w_{ij} / mwij=wij+ηΔwij/m\n",
    "- Repeat for ee epochs.\n",
    "\n",
    "## Backpropagation exercise\n",
    "\n",
    "Now you're going to implement the backprop algorithm for a network trained on the graduate school admission data. You should have everything you need from the previous exercises to complete this one.\n",
    "\n",
    "Your goals here:\n",
    "\n",
    "- Implement the forward pass.\n",
    "- Implement the backpropagation algorithm.\n",
    "- Update the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "admissions = pd.read_csv('binary.csv')\n",
    "\n",
    "# Make dummy variables for rank\n",
    "data = pd.concat([admissions, pd.get_dummies(admissions['rank'], prefix='rank')], axis=1)\n",
    "data = data.drop('rank', axis=1)\n",
    "\n",
    "# Standarize features\n",
    "for field in ['gre', 'gpa']:\n",
    "    mean, std = data[field].mean(), data[field].std()\n",
    "    data.loc[:,field] = (data[field]-mean)/std\n",
    "    \n",
    "# Split off random 10% of the data for testing\n",
    "np.random.seed(21)\n",
    "sample = np.random.choice(data.index, size=int(len(data)*0.9), replace=False)\n",
    "data, test_data = data.ix[sample], data.drop(sample)\n",
    "\n",
    "# Split into features and targets\n",
    "features, targets = data.drop('admit', axis=1), data['admit']\n",
    "features_test, targets_test = test_data.drop('admit', axis=1), test_data['admit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.25135725242598617\n",
      "Train loss:  0.24996540718842886\n",
      "Train loss:  0.24862005218904654\n",
      "Train loss:  0.24731993217179746\n",
      "Train loss:  0.24606380465584848\n",
      "Train loss:  0.24485044179257162\n",
      "Train loss:  0.2436786320186832\n",
      "Train loss:  0.24254718151769536\n",
      "Train loss:  0.24145491550165465\n",
      "Train loss:  0.24040067932493367\n",
      "Prediction accuracy: 0.725\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#from data_prep import features, targets, features_test, targets_test\n",
    "\n",
    "np.random.seed(21)\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "n_hidden = 2  # number of hidden units\n",
    "epochs = 900\n",
    "learnrate = 0.005\n",
    "\n",
    "n_records, n_features = features.shape\n",
    "last_loss = None\n",
    "# Initialize weights\n",
    "weights_input_hidden = np.random.normal(scale=1 / n_features ** .5,\n",
    "                                        size=(n_features, n_hidden))\n",
    "weights_hidden_output = np.random.normal(scale=1 / n_features ** .5,\n",
    "                                         size=n_hidden)\n",
    "\n",
    "for e in range(epochs):\n",
    "    del_w_input_hidden = np.zeros(weights_input_hidden.shape)\n",
    "    del_w_hidden_output = np.zeros(weights_hidden_output.shape)\n",
    "    for x, y in zip(features.values, targets):\n",
    "        ## Forward pass ##\n",
    "        # TODO: Calculate the output\n",
    "        hidden_input = np.dot(x, weights_input_hidden)\n",
    "        hidden_output = sigmoid(hidden_input)\n",
    "\n",
    "        output = sigmoid(np.dot(hidden_output,\n",
    "                                weights_hidden_output))\n",
    "\n",
    "        ## Backward pass ##\n",
    "        # TODO: Calculate the network's prediction error\n",
    "        error = y - output\n",
    "\n",
    "        # TODO: Calculate error term for the output unit\n",
    "        output_error_term = error * output * (1 - output)\n",
    "\n",
    "        ## propagate errors to hidden layer\n",
    "\n",
    "        # TODO: Calculate the hidden layer's contribution to the error\n",
    "        hidden_error = np.dot(output_error_term, weights_hidden_output)\n",
    "\n",
    "        # TODO: Calculate the error term for the hidden layer\n",
    "        hidden_error_term = hidden_error * hidden_output * (1 - hidden_output)\n",
    "\n",
    "        # TODO: Update the change in weights\n",
    "        del_w_hidden_output += output_error_term * hidden_output\n",
    "        del_w_input_hidden += hidden_error_term * x[:, None]\n",
    "\n",
    "    # TODO: Update weights\n",
    "    weights_input_hidden += learnrate * del_w_input_hidden / n_records\n",
    "    weights_hidden_output += learnrate * del_w_hidden_output / n_records\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        hidden_output = sigmoid(np.dot(x, weights_input_hidden))\n",
    "        out = sigmoid(np.dot(hidden_output,\n",
    "                             weights_hidden_output))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "hidden = sigmoid(np.dot(features_test, weights_input_hidden))\n",
    "out = sigmoid(np.dot(hidden, weights_hidden_output))\n",
    "predictions = out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
